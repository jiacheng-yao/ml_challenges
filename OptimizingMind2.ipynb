{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli/\"> <img src=\"https://developer.download.nvidia.com/training/images/DLI%20Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Necessity of Explainability\n",
    "\n",
    "Due to the success of Artificial Intelligence, it is used in a diverse range of applications, from song selection to self-driving cars. Depending on the task, an incorrect decision made by the AI may have unpleasant or even deadly consequences. Consider, for instance, the tragedy in March 2018 where an Uber self-driving car failed to recognize a pedestrian crossing the street.\n",
    "\n",
    "This raises some questions. Who is at fault when an AI system makes a mistake? How can these mistakes be predicted and avoided? And how can the AI be troubleshot and repaired?\n",
    "\n",
    "Unfortunately, it is difficult to understand what AI is really recognizing. It may not be looking for what you expect and can be easily fooled.\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"img/image.png\" width=\"600px\" />\n",
    "\n",
    "**Fig. 1:** Adding slight noise to an image – undetectable by humans – can cause the AI to make significant errors. The left side shows images correctly recognized (recognized label shown below each image). The right side shows these images slightly modified and no longer correctly recognized by the AI (image from Papernot et al 2016).\n",
    "\n",
    "---\n",
    "\n",
    "If people responsible for the AI system cannot fully understand what the AI is looking for, these unexpected errors will continue to occur and cost money, or worse – lives.\n",
    "\n",
    "# Important concepts from this tutorial:\n",
    "\n",
    "## 1. Accuracy does not guarantee your AI is error free\n",
    "\n",
    "Even if a classifier based on a Neural Network has excellent accuracy scores, it may not recognize in ways we expect and potentially fail when deployed. This is because accuracy measures performance on the data set, which is not a complete representation of the real world, and is likely to be flawed in a non-obvious way. High accuracy is usually expressed as a high correct percentage or low error rate.\n",
    "\n",
    "## 2. Data cleaning can cause unintended errors\n",
    "\n",
    "Datasets are highly manipulated and cleaned. The choice of data samples and how they are normalized can significantly alter recognition results. Understanding the model and the patterns it obtains from the data is an integral part of debugging AI.\n",
    "\n",
    "## 3. The patterns the network is looking for can be directly observed\n",
    "\n",
    "We introduce a way to derive what each node of the network is looking for, directly from the network's hidden internals. From the examples in this lab:\n",
    "\n",
    "- An explanation reveals never-before-seen, unexpected patterns learned from the data set.\n",
    "- You will have a chance to verify that these patterns are correct.\n",
    " \n",
    "## 4. Explaining by trial-and-error will miss problems\n",
    "\n",
    "Searching for what the problem could be by trial-and-error is:\n",
    "\n",
    "- Opaque. The internals of the AI still remain hidden – as before, it is a black box.\n",
    "- Incomplete. Explanations only cover the possibilities guessed: if a possibility is not tried, it will not be reported.\n",
    "- Time-consuming. Each explanation instance has to go through a trial and error set, which cannot cover all possibilities.\n",
    "\n",
    "\n",
    "## 5. Even a single-layer linear network can hide unreasonable decisions\n",
    "\n",
    "In this lab you will see that even a simple network with one layer can hide unreasonable decision criteria. And if one layer is enough to be unexplainable, then a stack of many such layers is even more so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Mind Registration\n",
    "\n",
    "In this lab, we will demonstrate _illumination_ of a neural network AI. To do this, you have a choice:\n",
    "\n",
    "- Use Optimizing Mind's Illuminated AI service complementary for this lab. You will be able to explain the network you train. To use this service, sign up below (or log in if you previously signed up).\n",
    "- Use static example. This lab includes a pre-generated illumination of a neural network slightly different than the one you train (due to initial randomization of weights). To use static explanations, simply skip to the next section.\n",
    "\n",
    "Whichever option you choose, please provide your feedback at the end of the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import OM_Tools\n",
    "\n",
    "OM_Tools.User.login_form()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab overview\n",
    "\n",
    "In this lab we will see that even a simple network with one layer can hide unreasonable decision criteria. We will train a single-layer, linear neural network with no hidden layers, and see an explanation using Optimizing Mind's technology.\n",
    "\n",
    "We will see explanations on 3 different classifiers:\n",
    "\n",
    "- SVM (Support Vector Machine) using scikit-learn\n",
    "- Single-layer perceptron using scikit-learn\n",
    "- A neural network using Keras\n",
    "\n",
    "For each classifier we will:\n",
    "\n",
    "- Train on the same data\n",
    "- Evaluate recognition of single digits from the MNIST dataset\n",
    "- Calculate accuracy\n",
    "- Show what the classifier is looking for in each digit\n",
    "- Easily create adversarial examples\n",
    "\n",
    "We will also compare Optimizing Mind technology with other existing explanation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Fundamentals\n",
    "\n",
    "The basic equation of a neural network AI Classifier is $Y = W X$. The symbols in this equation are:\n",
    "\n",
    "- $X$: the input\n",
    "- $W$: the learned neural network weights\n",
    "- $Y$: the prediction or output\n",
    "\n",
    "This equation multiplies the inputs by a set of weights to generate the outputs. Depending on the application, this may be called recognition, classification, or inference. Within the equation, information passes in one direction – from the inputs to the outputs. This is also referred to as feed-forward, a feedforward class network, or a feedforward equation. The feedforward equation is the fundamental building block on which the vast majority of neural networks are based.\n",
    "\n",
    "Modifications and nonlinearities may be added to this equation in order to make other types of neural networks. For example, deep neural networks (DNNs) have multiple layers of this basic equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "Let's load the data set and functions necessary for graphing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train_orig, y_train), (x_test_orig, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = np.stack([x_train_orig]*3, axis=-1)   # convert data into RGB form for LIME\n",
    "x_test = np.stack([x_test_orig]*3, axis=-1)   # convert data into RGB form for LIME\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('MNIST database loaded:')\n",
    "print(' - ', x_train.shape[0], 'train samples')\n",
    "print(' - ', x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_oh_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_oh_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "flat_x_train = np.reshape(x_train_orig,(x_train_orig.shape[0], 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_image_and_intended_activation(image, activation, string):   # plotting function\n",
    "    #show image\n",
    "    num_Digits = 10   #number of labels \n",
    "    lbl_txt_py = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "    plt.subplot(1,2,2)  #right side of figure\n",
    "    if image.shape[0] == 28:  #if image is 28x28\n",
    "        plt.imshow(image, extent=[0, 1, 0, 1]) #show image as 28x28\n",
    "    else:  #if image is flat 784\n",
    "        plt.imshow(np.reshape(image,(28,28)), extent=[0, 1, 0, 1]) #show image as 28x28\n",
    "    plt.axis('off') \n",
    "    plt.title(string) \n",
    "\n",
    "    #plot activation\n",
    "    plt.subplot(1,2,1)  #left side of figure\n",
    "    plt.bar(np.arange(num_Digits), activation)  # plot activations\n",
    "    plt.xticks(np.arange(num_Digits), lbl_txt_py)\n",
    "    plt.xlabel('Neuron')\n",
    "    plt.ylabel('Activation')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the data set\n",
    "We will train this single layer network on the MNIST data set, one of the most simple and intuitive datasets available.\n",
    "\n",
    "The MNIST database is a dataset of handwritten digits. It has 60,000 training samples, and 10,000 test samples. Each image is of size of 28x28 pixels and has 10 outputs each representing the digits 1 through 9 and 0. \n",
    "\n",
    "<img src = \"img/mnist-input.png\" width=600px;>\n",
    "\n",
    "Let's familiarize ourselves with this data set. \n",
    "Choose a sample and display it. \n",
    "The code will show the image (on right) and value supplied by the label (on left)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "observe_image_number = int(input('Choose training image number from 0 to 60,000: '))\n",
    "\n",
    "ind = y_train[observe_image_number] \n",
    "string = str('This image is to be trained as: '+str(ind))\n",
    "\n",
    "plot_image_and_intended_activation(x_train[observe_image_number], y_oh_train[observe_image_number], string) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What went into preparing the data set?\n",
    "Preparing a data is easily the most arduous task of developing AI.  Datasets must be carefully curated and cleaned.\n",
    "An important data requirement is that there must be about the same number of samples of each digit.  If there are not an equal number, data scientists generate more data that is \"similar\" to existing data.<br>\n",
    "\n",
    "Data set preparation issues cause many errors in AI.  <br>\n",
    "\n",
    "This dataset like any other is extensively curated and cleaned.  \n",
    "Can we directly observe some of this data cleaning?\n",
    "Let's see what the average digit looks like in each category.\n",
    "\n",
    "Let's take a look at the average digits of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for select_digit in range(num_classes):   # select digit\n",
    "    label = np.zeros((10))   \n",
    "    label[select_digit] = 1   # create label like it is found in the data set\n",
    "    index_of_select_digit = np.where(y_train == select_digit)[0]\n",
    "\n",
    "    mean_image_of_select_digit = (np.mean(x_train[index_of_select_digit,:,:,0], axis=0))  # average all selected digit \n",
    "    plt.subplot(2, 5, select_digit+1)  #right side of figure\n",
    "    plt.imshow(np.reshape(mean_image_of_select_digit, (28,28)), extent=[0, 1, 0, 1]) #show image as 28x28\n",
    "    plt.axis('off') \n",
    "    plt.title(select_digit) \n",
    "plt.suptitle('Average Digit from Data Set')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average data for each digit label looks like a prototypical digit, which is reassuring. But note some oddities – certain digits are higher than others. For example, 7 and 9 are lower than 6. Digits do not usually appear this way.\n",
    "\n",
    "### What is going on?\n",
    "\n",
    "It looks like the authors of this data set possibly used a function to normalize the image by the weight of its pixels. They may have averaged all the pixels for each image then centered the images at their center of mass. That could cause the 7s and 9s to be higher and 6s to be lower, because their pixel-wise \"center of mass\" is different.\n",
    "\n",
    "Can these subtle changes affect recognition? We are now almost ready to train the network and see.\n",
    "\n",
    "But one more step remains. After the data set is cleaned, it must be sorted randomly and digits presented in fixed frequencies otherwise known as independent and identically distributed (IID) random variables. If the data set is not sorted in this manner the network will suffer from catastrophic interference (i.e. forget some of the digits during training).\n",
    "\n",
    "Now let's train the simple single-layer neural network on the MNIST digit recognition data set. The network accepts the data set images of size 28x28 and has 10 outputs each representing the digits 1 through 9 and 0 and flattens them.\n",
    "\n",
    "- We will train this single-layer network using several methods.\n",
    "- We will use the OM illumination technology to see what we can understand about how the networks are recognizing the digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier 1 - SVM (Support Vector Machine) using scikit-learn\n",
    "\n",
    "## Training\n",
    "\n",
    "We skip every other training sample to reduce training time to approximately one minute. Feel free to experiment with the training if you registered for Optimizing Mind tools earlier in this lab. You may register anytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "svm = LinearSVC().fit(flat_x_train[::2], y_train[::2])  # ::2 skips every other training example\n",
    "end = time.time()\n",
    "print('Score on training',svm.score(flat_x_train, y_train))\n",
    "print('Training time = {0} sec'.format(int(end-start)))\n",
    "\n",
    "#Predict Output\n",
    "yhat_test_svm = svm.predict(np.reshape(x_test_orig,(x_test_orig.shape[0],784)))\n",
    "#Get weights\n",
    "svm_weights = svm.coef_.T "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM recognition of individual images\n",
    "\n",
    "Let’s explicitly do recognition on images one at a time. This is commonly abstracted away in within the predict function of learning tools but is the fundamental processing of all feedforward neural networks.\n",
    "\n",
    "We then pick the activation of the most positive neuron which represents what the network has decided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_image_number = int(input('Choose test image number from 0 to 10,000: '))\n",
    "\n",
    "test_image = x_test[test_image_number,:]\n",
    "net_input = np.reshape(test_image[:,:,0],(784))  #reshape into 784 vector\n",
    "NN_recognition = np.dot(net_input, svm_weights)  #calculate neuron activations based on y=wx\n",
    "ind = np.unravel_index(np.argmax(NN_recognition, axis=None), NN_recognition.shape)  #find most active node\n",
    "\n",
    "string=str('Test image #'+str(test_image_number))\n",
    "print('')\n",
    "print('         AI thinks this is a {0}'.format(ind[0]))\n",
    "print('         (Test set marked it as a {0})'.format(y_test[test_image_number]))\n",
    "plot_image_and_intended_activation(test_image, NN_recognition, string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Accuracy\n",
    "\n",
    "Now let's test how well the network \"recognizes overall\" by calculating its accuracy. In the code below, we run through the entire test set and count how many results are correct to get accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_correct = 0\n",
    "for x in range(x_test.shape[0]):\n",
    "       if yhat_test_svm[x] == y_test[x]: # we use the predict function of the learning library\n",
    "              num_correct += 1\n",
    "print('Percent Accuracy on Test Set = ',(100 * num_correct) / x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is relatively high. This must be a good network, right? The higher the accuracy, the less errors the network will do in real life, right?\n",
    "\n",
    "Not so fast! This is a common mistake that even experts make. Let’s see what the network is actually looking for..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Mind Explanation\n",
    "\n",
    "Next, let's reveal the ideal patterns the network is looking for using Optimizing Mind's technology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import OM_Tools\n",
    "\n",
    "if OM_Tools.User.logged_in():\n",
    "    ret = OM_Tools.User.weights_to_patterns(svm_weights)\n",
    "    if ret.return_code == 'Success':\n",
    "        print('Optimizing Mind API completed successfully.')\n",
    "        svm_patterns = ret.output\n",
    "    else:\n",
    "        print('{0}: {1}'.format(ret.return_code, ret.error))\n",
    "else:\n",
    "    svm_patterns = np.load('patterns/svm_patterns.npy')\n",
    "    print('Static patterns loaded successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prove that these are the ideal patterns because when we run them through the network as inputs, they activate only the associated digit and no others. The network reports 100% for each of these patterns.\n",
    "\n",
    "_Note that if you signed in with Optimizing Mind earlier in this lab, you will see a 100% accurate representation of what your network is looking for here. If not, you will have approximate static patterns for a network which will be similar to the one you trained (but not exactly the same)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Scroll down to see all patterns.')\n",
    "for digit in range(0,10):\n",
    "    test_image = svm_patterns[digit,:]   # image chosen from the test set based on image number\n",
    "    NN_recognition = np.dot(svm_weights.T, test_image)  #calculate neuron activations\n",
    "    string=str('Ideal Pattern for {0}'.format(digit))\n",
    "\n",
    "    plot_image_and_intended_activation(test_image, NN_recognition,string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is going on?\n",
    "\n",
    "The network learned from the data set that these pixels are most predictive patterns for the numbers. However, these patterns are very different from digits.\n",
    "\n",
    "Some of these results may be due to the data cleaning that set certain numerals at certain heights.\n",
    "\n",
    "### Old Military AI story\n",
    "\n",
    "There is a story about old military exploration of neural networks.  They trained their AI to distinguish between friendly tanks and enemy tanks. The AI worked well for the test and training set, but completely failed when when they deployed the AI in the field.\n",
    "\n",
    "They pondered the reason until someone realized that all of the pictures of enemy tanks were taken on a cloudy day while all of the friendly tanks were taken on a sunny day. So what did the network learn?\n",
    "\n",
    "The network learned to distinguish between cloudy and sunny days! The learning algorithm latched onto a systematic feature unintentionally introduced into the data set — in this case, the sky and clouds.\n",
    "\n",
    "### This is not a solved problem!\n",
    "\n",
    "As data becomes more complex, there are even more possibilities of systematic errors. They are impossible to guess, so the only way is to observe the patterns the AI is looking for and evaluate if those patterns make sense. In such cases, “accuracy” gives false expectations and having true explainability is essential.\n",
    "\n",
    "Such unintended systematic features can occur in many shapes and forms. In our case, they are pixels in certain unusual locations associated with certain digits.\n",
    "\n",
    "There are several other examples of datasets that are known to be faulty and some are listed here: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2529479/#S3title. In the early days of facial recognition, for instance, facial datasets could give accurate results even with the faces completely blotted out! The Essex facial dataset introduces a high classification accuracy of 97% when using the non-facial areas of the 42×100 top left pixels of each image.\n",
    "\n",
    "\n",
    "# Creating adversarial examples for SVM\n",
    "\n",
    "Once we know the patterns the network is looking for, we can use this information to easily create adversarial examples. We simply pick the strongest pixels and present this to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "percent = 20/100  # Choose the strongest x% pixels  \n",
    "edged = 1 - percent\n",
    "\n",
    "print('Scroll down to see all patterns.')\n",
    "for digit in range(0,10):\n",
    "    top = np.max(svm_patterns[digit,:])  # find max and min pixels in image\n",
    "    bottom = np.min(svm_patterns[digit,:])\n",
    "    adversarial_image = (svm_patterns[digit,:] * (svm_patterns[digit,:] > (edged * top)) \n",
    "                         + svm_patterns[digit,:] * (svm_patterns[digit,:] < (edged * bottom)))  # include pixels within range of max  \n",
    "        \n",
    "    NN_recognition = np.dot(svm_weights.T,adversarial_image)  #calculate neuron activations\n",
    "    \n",
    "    string = str('Adversarial Pattern for {}'.format(digit))\n",
    "    plot_image_and_intended_activation(adversarial_image, NN_recognition,string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These patterns look nothing like digits! Yet the network, having learned these unexpected patterns, treats them like digits nonetheless. If this AI is released as a digit detector in the real world, it will surely make mistakes.\n",
    "\n",
    "## What is going on?\n",
    "\n",
    "There appears to be systematic noise in the data set.  \n",
    "\n",
    "For instance, if a certain pixel is on only for a specific digit in the data set (e.g. the top right corner of the image) it can become highly predictive for that digit. However, these specific pixels would only be predictive in that specific data set.\n",
    "\n",
    "Such errors contribute to the colorful history of problems in datasets, like the early facial recognition data set having enough differences in the background that the faces themselves do not matter.\n",
    "\n",
    "The fact that some digits are artificially lower in the image likely also contributes to systematic noise: pixels not within the digits are overly predictive. A similar error would be a network learning to recognize a fox by looking at snow instead of the animal, similar to the clouds above the tank example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How can this be fixed?\n",
    "\n",
    "As a general rule, adding more data from various sources to the training should help. However even then, there is no guarantee that the data is free from systematic noise (e.g. an unintended single pixel in the corner being very predictive in the training set). Fortunately, the explanation not only reveals insights about what is in the network, but also the data set.\n",
    "\n",
    "Optimizing Mind has a method to train these types of networks directly. See the MNIST summary section to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME on SVM\n",
    "\n",
    "We discussed that accuracy will not reveal what patterns the network looks for and good accuracy may not correlate to recognition using real world patterns. Now let's see how other methods of explainability reveal patterns the network learned. We will evaluate the same network with LIME which works by randomly modifying input images to determine which patterns the network is more sensitive to.  \n",
    "\n",
    "A great primer on LIME (Local Interpretable Model-Agnostic Explanations) and the need for explainability can be found here: https://www.youtube.com/watch?v=hUnRCxnydCc\n",
    "\n",
    "For the LIME part of the demonstration we will use the LIME libraries and scikit-learn.\n",
    "\n",
    "We will follow the flow as described in https://github.com/marcotcr/lime/blob/master/doc/notebooks/Tutorial%20-%20MNIST%20and%20RF.ipynb (using the same LinearSVC as we do to train in our example).\n",
    "\n",
    "We will use a pipeline for processing the images as LIME requires images to be flattened and in RGB format. Then we ask, does running LIME on the same model reveal the same problems we found earlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from skimage.color import gray2rgb,rgb2gray, label2rgb\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "class PipeStep(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Wrapper for turning functions into pipeline transforms (no-fitting)\n",
    "    *** This should be linear, just returning the values\n",
    "    \"\"\"\n",
    "    def __init__(self, step_func):\n",
    "        self._step_func = step_func\n",
    "        \n",
    "    def fit(self, *args):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self._step_func(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_preproc = PipeStep(lambda img_list: rgb2gray(img_list.reshape(img_list.shape[0], -1)))\n",
    "svc = CalibratedClassifierCV(LinearSVC())\n",
    "svc_pipe = Pipeline([\n",
    "    ('Gray Flatten Image', img_preproc),\n",
    "    ('SVM', svc)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to retrain the same data in grayscale RGB format. Training will take about 90 seconds or more. Might be a good time to get up and stretch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "start = time.time()\n",
    "svc_pipe.fit(x_train[::2], y_train[::2])  # ::2 skips every other training example\n",
    "end = time.time()\n",
    "print('Training time = {0} sec'.format(int(end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yhat_test_svc = svc_pipe.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os,sys\n",
    "try:\n",
    "    import lime\n",
    "except:\n",
    "    sys.path.append(os.path.join('..', '..')) # add the current directory\n",
    "    import lime\n",
    "\n",
    "from lime import lime_image\n",
    "from lime.wrappers.scikit_image import SegmentationAlgorithm\n",
    "explainer = lime_image.LimeImageExplainer(verbose = False)\n",
    "segmenter = SegmentationAlgorithm('quickshift', kernel_size=1, max_dist=200, ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img_idx = int(input('Choose test image number from 0 to 10,000: '))\n",
    "\n",
    "explanation = explainer.explain_instance(x_test[img_idx], \n",
    "                                         classifier_fn = svc_pipe.predict_proba, \n",
    "                                         top_labels=10, hide_color=0, num_samples=10000, segmentation_fn=segmenter)\n",
    "\n",
    "\n",
    "temp, mask = explanation.get_image_and_mask(y_test[img_idx], positive_only=True, \n",
    "                                            num_features=10, hide_rest=False, min_weight = 0.01)\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (8, 4))\n",
    "ax1.imshow(label2rgb(mask,temp, bg_label = 0), interpolation = 'nearest')\n",
    "ax1.set_title('Positive Regions for {}'.format(y_test[img_idx]))\n",
    "temp, mask = explanation.get_image_and_mask(y_test[img_idx], positive_only=False, \n",
    "                                            num_features=10, hide_rest=False, min_weight = 0.01)\n",
    "ax2.imshow(label2rgb(3-mask,temp, bg_label = 0), interpolation = 'nearest')\n",
    "ax2.set_title('Positive/Negative Regions for {}'.format(y_test[img_idx]))\n",
    "\n",
    "# now show them for each class\n",
    "fig, m_axs = plt.subplots(2,5, figsize = (12,6))\n",
    "for i, c_ax in enumerate(m_axs.flatten()):\n",
    "    temp, mask = explanation.get_image_and_mask(i, positive_only=True, \n",
    "                                                num_features=1000, hide_rest=False, min_weight = 0.01 )\n",
    "    c_ax.imshow(label2rgb(mask, x_test[img_idx], bg_label = 0), interpolation = 'nearest')\n",
    "    c_ax.set_title('Positive for {}\\nActual {}'.format(i, y_test[img_idx]))\n",
    "    c_ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods like LIME vary inputs and see what the network is more sensitive to. This is based on trial and error and may miss certain variations of inputs if the right variations are not programmed in to try.\n",
    "\n",
    "Optimizing Mind technology avoids guessing. Optimizing Mind uses a unique, direct, and efficient solution that reveals the ground-truth, and provides the patterns which the network is actually looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier 2 - Single-layer perceptron using scikit-learn\n",
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import time\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=()) # use single layer training \n",
    "start = time.time()\n",
    "mlp.fit(flat_x_train, y_train)\n",
    "end = time.time()\n",
    "print('Score on training',mlp.score(flat_x_train, y_train))\n",
    "print('Training time = {0} sec'.format(int(end-start)))\n",
    "\n",
    "#Predict Output\n",
    "yhat_test_mlp = mlp.predict(np.reshape(x_test_orig, (x_test_orig.shape[0],784)))\n",
    "#Get weights\n",
    "mlp_weights = mlp.coefs_[0].T "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-layer perceptron recognition of individual images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_image_number = int(input('Choose test number from 0 to 10,000: '))\n",
    "\n",
    "test_image = x_test[test_image_number,:] \n",
    "net_input = np.reshape(test_image[:,:,0], (784))  #reshape into 784 vector\n",
    "NN_recognition = np.dot(net_input.T, mlp_weights.T)  #calculate neuron activations based on y=wx\n",
    "ind = np.unravel_index(np.argmax(NN_recognition, axis=None), NN_recognition.shape)  #find most active node\n",
    "\n",
    "string=str('Test number '+str(test_image_number)) \n",
    "print('         AI thinks this is a {0}'.format(ind[0]))\n",
    "print('         (Test set marked it as a {0})'.format(y_test[test_image_number]))\n",
    "plot_image_and_intended_activation(test_image, NN_recognition, string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_correct = 0\n",
    "for x in range(x_test.shape[0]):\n",
    "       if yhat_test_mlp[x] == y_test[x]: # we use the predict function of the learning library\n",
    "              num_correct += 1\n",
    "print('Percent Accuracy on Test Set = ', (100 * num_correct) / x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at the confusion matrix which tells us how many errors of different types occur.\n",
    "\n",
    "Entries on the diagonal are correct recognitions. Entries above the diagonals are false positives, and the ones on the bottom are false negatives. For more information, see: https://en.wikipedia.org/wiki/Confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, yhat_test_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see what the network is looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Mind Explanation\n",
    "\n",
    "Next, let's reveal the ideal patterns the network is looking for using Optimizing Mind's technology.  \n",
    "\n",
    "_Note that if you signed in with Optimizing Mind and use our tool you will have a 100% accurate display. If not, you will have approximate static patterns of a network which should be similar to the one you trained (but not exact)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import OM_Tools\n",
    "\n",
    "if OM_Tools.User.logged_in():\n",
    "    ret=OM_Tools.User.weights_to_patterns(mlp_weights.T)\n",
    "    if ret.return_code == 'Success':\n",
    "        print('Optimizing Mind API completed successfully.')\n",
    "        mlp_patterns = ret.output\n",
    "    else:\n",
    "        print('{0}: {1}'.format(ret.return_code,ret.error))\n",
    "else:\n",
    "    mlp_patterns = np.load('patterns/mlp_patterns.npy')\n",
    "    print('Static patterns loaded successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Scroll down to see all patterns.')\n",
    "for digit in range(0,10):\n",
    "    test_image = mlp_patterns[digit,:]\n",
    "    NN_recognition = np.dot(mlp_weights, test_image)  #calculate neuron activations\n",
    "    string=str('Ideal Pattern for {}'.format(digit))\n",
    "\n",
    "    plot_image_and_intended_activation(test_image, NN_recognition,string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating perceptron adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "percent = 20/100  # Choose the strongest x% pixels  \n",
    "edged = 1 - percent\n",
    "\n",
    "print('Scroll down to see all patterns.')\n",
    "for digit in range(0,10):\n",
    "    top = np.max(mlp_patterns[digit,:])  # find max and min pixels in image\n",
    "    bottom = np.min(mlp_patterns[digit,:])\n",
    "    adversarial_image = (mlp_patterns[digit,:] * (mlp_patterns[digit,:] > (edged * top)) \n",
    "                         + mlp_patterns[digit,:] * (mlp_patterns[digit,:] < (edged * bottom)))  # include pixels within range of max  \n",
    "    NN_recognition = np.dot(mlp_weights, adversarial_image)  #calculate neuron activations\n",
    "\n",
    "    string = str('Adversarial Pattern for {}'.format(digit))\n",
    "    plot_image_and_intended_activation(adversarial_image, NN_recognition, string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the SVM and perceptron examples show similar, odd adversarial patterns. Let's try another network..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier 3 - A neural network using Keras\n",
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense, Flatten, Lambda, InputLayer\n",
    "\n",
    "keras = Sequential()\n",
    "keras.add(InputLayer((28,28,3)))\n",
    "keras.add(Lambda(lambda x: x[...,0]))\n",
    "keras.add(Flatten())\n",
    "keras.add(Dense(10, activation='softmax', use_bias=False))\n",
    "keras.compile('SGD', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "keras.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keras.fit(x_train, y_oh_train, epochs=7,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keras_weights = keras.layers[-1].get_weights()[0]\n",
    "yhat_test_keras = keras.predict_proba(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras recognition of individual images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_image_number = int(input('Choose test number from 0 to 10,000: '))\n",
    "\n",
    "test_image = x_test[test_image_number,:] \n",
    "net_input = np.reshape(test_image[:,:,0], (784))  #reshape into 784 vector\n",
    "NN_recognition = np.dot(keras_weights.T, net_input)  #calculate neuron activations based on y=wx\n",
    "ind = np.unravel_index(np.argmax(NN_recognition, axis=None), NN_recognition.shape)  #find most active node\n",
    "\n",
    "string=str('Test number '+str(test_image_number)) \n",
    "print('         AI thinks this is a {0}'.format(ind[0]))\n",
    "print('         (Test set marked it as a {0})'.format(y_test[test_image_number]))\n",
    "plot_image_and_intended_activation(test_image, NN_recognition, string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_correct = 0\n",
    "for x in range(x_test.shape[0]):\n",
    "    ind = np.argmax(yhat_test_keras[x])  # we use the predict function of the learning library\n",
    "    if ind == y_test[x]: \n",
    "        num_correct += 1\n",
    "print('Percent Accuracy on Test Set = ', (100 * num_correct) / (x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let’s see what the network is looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Mind Explanation\n",
    "\n",
    "Next, let’s reveal the ideal patterns the network is looking for using Optimizing Mind’s technology.\n",
    "\n",
    "_Note that if you signed in with Optimizing Mind and use our tool you will have a 100% accurate display. If not, you will have approximate static patterns of a network which should be similar to the one you trained (but not exact)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import OM_Tools\n",
    "\n",
    "if OM_Tools.User.logged_in():\n",
    "    ret=OM_Tools.User.weights_to_patterns(keras_weights)\n",
    "    if ret.return_code == 'Success':\n",
    "        print('Optimizing Mind API completed successfully.')\n",
    "        keras_patterns = ret.output\n",
    "    else:\n",
    "        print('{0}: {1}'.format(ret.return_code,ret.error))\n",
    "else:\n",
    "    keras_patterns = np.load('patterns/keras_patterns.npy')\n",
    "    print('Static patterns loaded successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Scroll down to see all patterns.')\n",
    "for digit in range(0,10):\n",
    "    test_image = keras_patterns[digit,:] \n",
    "    display_image = np.stack([test_image.reshape(28,28)]*3, axis=-1)\n",
    "    NN_recognition = np.dot(keras_weights.T, display_image[:,:,0].ravel())  #calculate neuron activations\n",
    "\n",
    "    string=str('Ideal Pattern for {}'.format(digit))\n",
    "    plot_image_and_intended_activation(display_image[:,:,0], NN_recognition, string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating adversarial examples for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "percent = 10/100  # Choose the strongest x% pixels  \n",
    "edged = 1 - percent\n",
    "\n",
    "print('Scroll down to see all patterns.')\n",
    "for digit in range (0,10):\n",
    "    top = np.max(keras_patterns[digit,:])  # find max and min pixels in image\n",
    "    bottom = np.min(keras_patterns[digit,:])\n",
    "    adversarial_image = (keras_patterns[digit,:] * (keras_patterns[digit,:] > (edged * top))\n",
    "                         + keras_patterns[digit,:] * (keras_patterns[digit,:] < (edged * bottom)))  # include pixels within range of max  \n",
    "\n",
    "    display_image = np.stack([adversarial_image.reshape(28,28)]*3, axis=-1)\n",
    "    NN_recognition = np.dot(keras_weights.T, display_image[:,:,0].ravel())  #calculate neuron activations\n",
    "\n",
    "    string=str('Adversarial Pattern for {}'.format(digit))\n",
    "    plot_image_and_intended_activation(display_image[:,:,0],NN_recognition, string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method does better that the previous two methods. For certain digits, it appears to be looking for expected digit patterns familiar to humans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Summary\n",
    "\n",
    "Below, we compare all 3 models we trained on each digit using the Optimizing Mind Illumination tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Scroll down to see all patterns.')\n",
    "for digit in range(0,10):\n",
    "    plt.subplot(1,3,1)  #organize all sub images into one figure\n",
    "    plt.imshow(np.reshape(svm_patterns[digit,:],(28,28)), extent=[0, 1, 0, 1]) #plot the pattern\n",
    "    plt.axis('off') \n",
    "    \n",
    "    plt.title('SVM') \n",
    "    plt.subplot(1,3,2)  #organize all sub images into one figure\n",
    "    plt.imshow(np.reshape(mlp_patterns[digit,:],(28,28)), extent=[0, 1, 0, 1]) #plot the pattern\n",
    "    plt.axis('off') \n",
    "    plt.title('MLP') \n",
    "    plt.subplot(1,3,3)  #organize all sub images into one figure\n",
    "    plt.imshow(np.reshape(keras_patterns[digit,:],(28,28)), extent=[0, 1, 0, 1]) #plot the pattern\n",
    "    plt.axis('off') \n",
    "    plt.title('Keras NN') \n",
    "    \n",
    "    string = str('What different networks look for in digit {}'.format(digit))\n",
    "    plt.suptitle(string)    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different models process digits differently, sometimes relying on data artifacts instead of sensible digits. The Keras library learning did well compared to the SVM and perceptron library methods in learning sensible patterns.\n",
    "\n",
    "However, even the Keras network was not complete. It produced sensible patterns for digits 0 and 1 but not digits 5 and 4.\n",
    "\n",
    "### More explainability\n",
    "\n",
    "We revealed the patterns the network is looking for. Using OM technology it is also possible to reveal which inputs are most important for any decision. This additional functionality is demonstrated in our video along with OM’s method of learning.\n",
    "\n",
    "### Optimizing Mind Learning based on expected patterns\n",
    "\n",
    "Being aware of potential adversarial examples and problems in our data set helps create better datasets, learning functions, and guides us on how to limit the function of AI to scenarios that make sense.\n",
    "There is also a way to train the AI on expected patterns avoiding the adversarial patterns. In addition, this avoids some of the difficulties of feedforward learning. We offer all of these solutions to our customers. See more in our video: https://www.youtube.com/watch?v=vKf0WnYEjrU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond the single layer\n",
    "\n",
    "The method we use derives the patterns internally from learned network weights and can be used for any type of feedforward neural network.\n",
    "\n",
    "This can be applied to deep and convolutional neural networks. We briefly show a sample of recognition of a ResNet deep network.\n",
    "\n",
    "Using the following image, the AI performed recognition below:\n",
    "\n",
    "<img src=\"img/test_image2.png\" width=\"500px\" />\n",
    "\n",
    "The recognition of the scene includes a person and a car (categories 11 and 13, respectively).\n",
    "\n",
    "<img src=\"img/pic-colored.png\" width=\"500px\" />\n",
    "\n",
    "This is derived from the output of the top layer:\n",
    "\n",
    "<img src=\"img/334_out2.png\" width=\"700px\" />\n",
    "\n",
    "We want a more detailed explanation and understanding of what it really takes to recognize a person. To do so, we choose a component representing a person and her location.\n",
    "\n",
    "We label the image as a person by making only image 11 of the output active. We can then ask, what is expected for a person that looks like this?\n",
    "\n",
    "<img src=\"img/labeled_Picture2.png\" width=\"700px\" />\n",
    "\n",
    "We can see which patterns the network is looking for in the 512 input images of that layer.\n",
    "\n",
    "<img src=\"img/334_pos_input_person_cropped.png\" width=\"900px\" />\n",
    "\n",
    "We can also see which patterns the network is looking for in the 4096 input images of the layer below.\n",
    "\n",
    "<img src=\"img/4096layer.png\" width=\"900px\" />\n",
    "\n",
    "Looks like having another person nearby (but only a certain distance) helps with the recognition at that layer.\n",
    "\n",
    "We can use this approach both in deep and convolutional neural networks.\n",
    "\n",
    "At Optimizing Mind, we are helping companies design their AI with better understanding: increasing confidence and decreasing mishaps. Similarly, we are assisting regulators and their customers to have the ability to get insights and trust for the AI they are looking to approve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback\n",
    "\n",
    "Thank you working through this lab. How did we do? Please answer this short questionnaire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import OM_Tools\n",
    "from IPython.display import IFrame\n",
    "\n",
    "email = OM_Tools.User.get_email() if OM_Tools.User.logged_in() else ''\n",
    "IFrame(src='https://optimizingmind.com/lab-feedback/?email=' + email, width='800px', height='700px')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to learn more, contact us at info@optimizingmind.com and visit our website: https://optimizingmind.com\n",
    "\n",
    "<img src=\"img/clear_blue_logo.png\" width=\"400px\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
